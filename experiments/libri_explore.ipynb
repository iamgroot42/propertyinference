{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d559bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "import distribution_inference.models.asr as models_asr\n",
    "import evaluate\n",
    "from distribution_inference.training.utils import load_model\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Dict, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperTokenizerFast\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75442c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_dir = \"/p/adversarialml/as9rw/datasets/librispeech/\"\n",
    "small_data_sample = load_from_disk(os.path.join(base_data_dir, \"processed\", \"adv\", \"audit_subjects\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add96648",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhisperTokenizerFast.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = small_data_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tokenizer(sam['text']).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429656af",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.generate(input_features=ch.Tensor(small_data_sample[:10]['input_features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d18704",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_str = tokenizer.batch_decode(z, skip_special_tokens=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24860ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(small_data_sample[:10]['text']).input_ids\n",
    "label_str = tokenizer.batch_decode(ids, skip_special_tokens=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad94de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07deb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, AirAbsorption, TanhDistortion\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "modify = small_data_sample[0]['audio']['array']\n",
    "ipd.Audio(modify, rate=16_000, autoplay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approved\n",
    "transform = AddGaussianNoise(\n",
    "    min_amplitude=0.01,\n",
    "    max_amplitude=0.015,\n",
    "    p=1.0\n",
    ")\n",
    "# Approved\n",
    "transform = PitchShift(\n",
    "    min_semitones=-4.0,\n",
    "    max_semitones=4.0,\n",
    "    p=1.0\n",
    ")\n",
    "# Approved\n",
    "transform = AirAbsorption(\n",
    "    min_distance=100,\n",
    "    max_distance=500,\n",
    "    p=1.0)\n",
    "# Approved\n",
    "transform = TanhDistortion(\n",
    "    min_distortion=0.1,\n",
    "    max_distortion=0.7,\n",
    "    p=1.0)\n",
    "\n",
    "augmented_sound = transform(modify, sample_rate=16_000)\n",
    "ipd.Audio(augmented_sound, rate=16_000, autoplay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50414253",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_feature = fe(augmented_sound, sampling_rate=16_000).input_features[0]\n",
    "z = model.generate(input_features=ch.Tensor([aug_feature]))\n",
    "pred_str = tokenizer.batch_decode(z, skip_special_tokens=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91560c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], ch.Tensor]]]) -> Dict[str, ch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]}\n",
    "                          for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]}\n",
    "                          for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd4622",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorSpeechSeq2SeqWithPadding(processor=WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\", language=\"en\", task=\"transcribe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = model(**collator([{\"input_features\": aug_feature, \"labels\": tokenizer(small_data_sample[0]['text']).input_ids}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "zz.loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a5b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute(predictions=pred_str, references=label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_changes_under_augs(data, tokenizer, batch_size: int, sample_rate: int = 16000):\n",
    "    # Approved\n",
    "    transforms = [\n",
    "        AddGaussianNoise(\n",
    "            min_amplitude=0.01,\n",
    "            max_amplitude=0.015,\n",
    "            p=1.0),\n",
    "        PitchShift(\n",
    "            min_semitones=-4.0,\n",
    "            max_semitones=4.0,\n",
    "            p=1.0),\n",
    "        AirAbsorption(\n",
    "            min_distance=100,\n",
    "            max_distance=500,\n",
    "            p=1.0),\n",
    "        TanhDistortion(\n",
    "            min_distortion=0.1,\n",
    "            max_distortion=0.7,\n",
    "            p=1.0)\n",
    "    ]\n",
    "    aug_data_flat = []\n",
    "    for x in tqdm(data, \"Generating augmented data\"):\n",
    "        aug_data_flat.extend([transform(x['audio']['array'], sample_rate) for transform in transforms])\n",
    "    aug_data_flat = ch.from_numpy(np.concatenate(aug_data_flat))\n",
    "    # Get encodings for text in data\n",
    "    all_text = data['text']\n",
    "    encodings = tokenizer(data['text']).input_ids\n",
    "    # Get model outputs for augmented data\n",
    "    wers = []\n",
    "    for i in range(0, len(aug_data_flat), batch_size):\n",
    "        batch = aug_data_flat[i:i+batch_size]\n",
    "        # Could make more efficient by only making forward call and using that to infer\n",
    "        # Generated sequence, but following is more fool-proof\n",
    "        \n",
    "        # Get loss values\n",
    "        collated_batch = \n",
    "        model()\n",
    "        \n",
    "        # Get outputs (for WER computation)\n",
    "        output = model.generate(input_features=batch.cuda())\n",
    "        pred_str = tokenizer.batch_decode(output, skip_special_tokens=True, normalize=True)\n",
    "        wers.append([metric.compute(predictions=pred, references=all_text[(i + j) // len(transforms)]) for j, pred in enumerate(pred_str)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f643608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = models_asr.WhisperTiny()\n",
    "model, (train_ids, _) = load_model(model, model_path, on_cpu=False)\n",
    "model.eval()\n",
    "print(\"Loaded model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0647e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract member speakers and their information\n",
    "pool_speakers = members[\"speaker_id\"]\n",
    "# Identify 'member' and 'non-member' data\n",
    "members_mask = np.where(np.isin(pool_speakers, train_ids))[0]\n",
    "members = members.select(members_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dac2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pick only N speakers from both sets and focus on their metrics\n",
    "def pick_speakers(ds, num: int):\n",
    "    unique_speakers = np.unique(ds[\"speaker_id\"])\n",
    "    picked_speakers = np.random.choice(unique_speakers, num, replace=False)\n",
    "    mask = np.where(np.isin(ds[\"speaker_id\"], picked_speakers))[0]\n",
    "    return ds.select(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d617300",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pick = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(subset_nonmembers[\"speaker_id\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03302da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_members    = pick_speakers(members, num_pick)\n",
    "subset_nonmembers = pick_speakers(non_members, num_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed27956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_to_device(data, device):\n",
    "    return {key: value.to(device) for key, value in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e43921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_metrics(m, features):\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "    input_features = [{\"input_features\": x} for x in features[\"input_features\"]]\n",
    "    batch = m.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "    # get the tokenized label sequences\n",
    "    label_features = [{\"input_ids\": x} for x in features[\"labels\"]]\n",
    "    # pad the labels to max length\n",
    "    labels_batch = m.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "    # replace padding with -100 to ignore loss correctly\n",
    "    labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "    labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "    # if bos token is appended in previous tokenization step,\n",
    "    # cut bos token here as it's append later anyways\n",
    "    if (labels[:, 0] == m.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "        labels = labels[:, 1:]\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    \n",
    "    # Get model output\n",
    "    with ch.no_grad():\n",
    "        batch_cuda = recursive_to_device(batch, \"cuda:0\")\n",
    "        logits = m.model(**batch_cuda).logits.cpu()\n",
    "        pred_ids = m.model.generate(**batch_cuda, max_length=225).cpu()\n",
    "        label_ids = batch[\"labels\"]\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = model.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = model.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    pred_str = [x.lstrip().strip() for x in pred_str]\n",
    "    label_str = model.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    wer = [wer_metric.compute(predictions=[x], references=[y]) for (x, y) in zip(pred_str, label_str)]\n",
    "    for x, y in zip(pred_str, label_str):\n",
    "        print(x)\n",
    "        print(y)\n",
    "        print()\n",
    "\n",
    "    # Compute loss\n",
    "    loss_function = ch.nn.CrossEntropyLoss()\n",
    "    losses = [loss_function(x.view(-1, x.shape[-1]), y.view(-1)).item() for (x, y) in zip(logits, label_ids)]\n",
    "\n",
    "    return wer, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae088906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(m, data, batch_size: int = 8):\n",
    "    all_metrics = []\n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        all_metrics.append(get_batch_metrics(m, data[i:i+batch_size]))\n",
    "        break\n",
    "    all_metrics = np.concatenate(all_metrics, 0).T\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c950efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_members = get_metrics(model, subset_members)\n",
    "losses_nonmembers = get_metrics(model, subset_nonmembers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_nonmembers[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss values, WER, and CER for both scenarios\n",
    "plt.hist(losses_members, 21, alpha=0.5, label=\"members\")\n",
    "plt.hist(losses_nonmembers, 21, alpha=0.5, label=\"non-members\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97122997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later: Consider adding noise/augmentations to input and measure robustness in model behavior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd9",
   "language": "python",
   "name": "phd9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
